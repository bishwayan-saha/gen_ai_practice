{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, Settings, VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client as qc\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.2\")\n",
    "embedding = OllamaEmbedding(model_name = \"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of document: 15\n"
     ]
    }
   ],
   "source": [
    "docs = SimpleDirectoryReader(input_files=[\"/home/bishwayansaha99/langchain/docs/attention.pdf\"]).load_data()\n",
    "print(f\"Length of document: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Sentence Text Splitter nodes: 15\n"
     ]
    }
   ],
   "source": [
    "default_node_parser = SentenceSplitter()\n",
    "default_nodes = default_node_parser.get_nodes_from_documents(docs)\n",
    "print(f\"Length of Sentence Text Splitter nodes: {len(default_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Sentence Window Splitter nodes: 372\n"
     ]
    }
   ],
   "source": [
    "sentence_window_parser = SentenceWindowNodeParser()\n",
    "sentence_window_nodes = sentence_window_parser.get_nodes_from_documents(docs)\n",
    "print(f\"Length of Sentence Window Splitter nodes: {len(sentence_window_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding': None,\n",
      " 'end_char_idx': 3188,\n",
      " 'excluded_embed_metadata_keys': ['file_name',\n",
      "                                  'file_type',\n",
      "                                  'file_size',\n",
      "                                  'creation_date',\n",
      "                                  'last_modified_date',\n",
      "                                  'last_accessed_date'],\n",
      " 'excluded_llm_metadata_keys': ['file_name',\n",
      "                                'file_type',\n",
      "                                'file_size',\n",
      "                                'creation_date',\n",
      "                                'last_modified_date',\n",
      "                                'last_accessed_date'],\n",
      " 'id_': 'ac45737f-8717-46e3-84b4-5f23bb2c22af',\n",
      " 'metadata': {'creation_date': '2025-01-03',\n",
      "              'file_name': 'attention.pdf',\n",
      "              'file_path': '/home/bishwayansaha99/langchain/docs/attention.pdf',\n",
      "              'file_size': 2215244,\n",
      "              'file_type': 'application/pdf',\n",
      "              'last_modified_date': '2025-01-01',\n",
      "              'page_label': '5'},\n",
      " 'metadata_separator': '\\n',\n",
      " 'metadata_seperator': '\\n',\n",
      " 'metadata_template': '{key}: {value}',\n",
      " 'mimetype': 'text/plain',\n",
      " 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ff1c34a0-fb2a-47ee-b6ab-bdd9950305a9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'attention.pdf', 'file_path': '/home/bishwayansaha99/langchain/docs/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-01-03', 'last_modified_date': '2025-01-01'}, hash='d2608ecd2dacf21af2b9e6227dfa731d263d9205eb638d6a1d6cb2094c32e4a2')},\n",
      " 'start_char_idx': 0,\n",
      " 'text': 'output values. These are concatenated and once again projected, '\n",
      "         'resulting in the final values, as\\n'\n",
      "         'depicted in Figure 2.\\n'\n",
      "         'Multi-head attention allows the model to jointly attend to '\n",
      "         'information from different representation\\n'\n",
      "         'subspaces at different positions. With a single attention head, '\n",
      "         'averaging inhibits this.\\n'\n",
      "         'MultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\n'\n",
      "         'where headi = Attention(QWQ\\n'\n",
      "         'i , KWK\\n'\n",
      "         'i , V WV\\n'\n",
      "         'i )\\n'\n",
      "         'Where the projections are parameter matricesWQ\\n'\n",
      "         'i ∈ Rdmodel×dk , WK\\n'\n",
      "         'i ∈ Rdmodel×dk , WV\\n'\n",
      "         'i ∈ Rdmodel×dv\\n'\n",
      "         'and WO ∈ Rhdv×dmodel .\\n'\n",
      "         'In this work we employ h = 8 parallel attention layers, or heads. '\n",
      "         'For each of these we use\\n'\n",
      "         'dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, '\n",
      "         'the total computational cost\\n'\n",
      "         'is similar to that of single-head attention with full '\n",
      "         'dimensionality.\\n'\n",
      "         '3.2.3 Applications of Attention in our Model\\n'\n",
      "         'The Transformer uses multi-head attention in three different ways:\\n'\n",
      "         '• In \"encoder-decoder attention\" layers, the queries come from the '\n",
      "         'previous decoder layer,\\n'\n",
      "         'and the memory keys and values come from the output of the encoder. '\n",
      "         'This allows every\\n'\n",
      "         'position in the decoder to attend over all positions in the input '\n",
      "         'sequence. This mimics the\\n'\n",
      "         'typical encoder-decoder attention mechanisms in sequence-to-sequence '\n",
      "         'models such as\\n'\n",
      "         '[38, 2, 9].\\n'\n",
      "         '• The encoder contains self-attention layers. In a self-attention '\n",
      "         'layer all of the keys, values\\n'\n",
      "         'and queries come from the same place, in this case, the output of '\n",
      "         'the previous layer in the\\n'\n",
      "         'encoder. Each position in the encoder can attend to all positions in '\n",
      "         'the previous layer of the\\n'\n",
      "         'encoder.\\n'\n",
      "         '• Similarly, self-attention layers in the decoder allow each '\n",
      "         'position in the decoder to attend to\\n'\n",
      "         'all positions in the decoder up to and including that position. We '\n",
      "         'need to prevent leftward\\n'\n",
      "         'information flow in the decoder to preserve the auto-regressive '\n",
      "         'property. We implement this\\n'\n",
      "         'inside of scaled dot-product attention by masking out (setting to '\n",
      "         '−∞) all values in the input\\n'\n",
      "         'of the softmax which correspond to illegal connections. See Figure '\n",
      "         '2.\\n'\n",
      "         '3.3 Position-wise Feed-Forward Networks\\n'\n",
      "         'In addition to attention sub-layers, each of the layers in our '\n",
      "         'encoder and decoder contains a fully\\n'\n",
      "         'connected feed-forward network, which is applied to each position '\n",
      "         'separately and identically. This\\n'\n",
      "         'consists of two linear transformations with a ReLU activation in '\n",
      "         'between.\\n'\n",
      "         'FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\n'\n",
      "         'While the linear transformations are the same across different '\n",
      "         'positions, they use different parameters\\n'\n",
      "         'from layer to layer. Another way of describing this is as two '\n",
      "         'convolutions with kernel size 1.\\n'\n",
      "         'The dimensionality of input and output is dmodel = 512, and the '\n",
      "         'inner-layer has dimensionality\\n'\n",
      "         'dff = 2048.\\n'\n",
      "         '3.4 Embeddings and Softmax\\n'\n",
      "         'Similarly to other sequence transduction models, we use learned '\n",
      "         'embeddings to convert the input\\n'\n",
      "         'tokens and output tokens to vectors of dimension dmodel. We also use '\n",
      "         'the usual learned linear transfor-\\n'\n",
      "         'mation and softmax function to convert the decoder output to '\n",
      "         'predicted next-token probabilities. In\\n'\n",
      "         'our model, we share the same weight matrix between the two embedding '\n",
      "         'layers and the pre-softmax\\n'\n",
      "         'linear transformation, similar to [30]. In the embedding layers, we '\n",
      "         'multiply those weights by √dmodel.\\n'\n",
      "         '5',\n",
      " 'text_template': '{metadata_str}\\n\\n{content}'}\n",
      "========================================================================================================================\n",
      "{'embedding': None,\n",
      " 'end_char_idx': 1166,\n",
      " 'excluded_embed_metadata_keys': ['file_name',\n",
      "                                  'file_type',\n",
      "                                  'file_size',\n",
      "                                  'creation_date',\n",
      "                                  'last_modified_date',\n",
      "                                  'last_accessed_date',\n",
      "                                  'window',\n",
      "                                  'original_text'],\n",
      " 'excluded_llm_metadata_keys': ['file_name',\n",
      "                                'file_type',\n",
      "                                'file_size',\n",
      "                                'creation_date',\n",
      "                                'last_modified_date',\n",
      "                                'last_accessed_date',\n",
      "                                'window',\n",
      "                                'original_text'],\n",
      " 'id_': '22173508-fe48-45f5-862d-70ba3e3fe334',\n",
      " 'metadata': {'creation_date': '2025-01-03',\n",
      "              'file_name': 'attention.pdf',\n",
      "              'file_path': '/home/bishwayansaha99/langchain/docs/attention.pdf',\n",
      "              'file_size': 2215244,\n",
      "              'file_type': 'application/pdf',\n",
      "              'last_modified_date': '2025-01-01',\n",
      "              'original_text': 'Experiments on two machine translation tasks '\n",
      "                               'show these models to\\n'\n",
      "                               'be superior in quality while being more '\n",
      "                               'parallelizable and requiring significantly\\n'\n",
      "                               'less time to train. ',\n",
      "              'page_label': '1',\n",
      "              'window': 'Attention Is All You Need\\n'\n",
      "                        'Ashish Vaswani∗\\n'\n",
      "                        'Google Brain\\n'\n",
      "                        'avaswani@google.com\\n'\n",
      "                        'Noam Shazeer∗\\n'\n",
      "                        'Google Brain\\n'\n",
      "                        'noam@google.com\\n'\n",
      "                        'Niki Parmar∗\\n'\n",
      "                        'Google Research\\n'\n",
      "                        'nikip@google.com\\n'\n",
      "                        'Jakob Uszkoreit∗\\n'\n",
      "                        'Google Research\\n'\n",
      "                        'usz@google.com\\n'\n",
      "                        'Llion Jones∗\\n'\n",
      "                        'Google Research\\n'\n",
      "                        'llion@google.com\\n'\n",
      "                        'Aidan N. Gomez∗ †\\n'\n",
      "                        'University of Toronto\\n'\n",
      "                        'aidan@cs.toronto.edu\\n'\n",
      "                        'Łukasz Kaiser∗\\n'\n",
      "                        'Google Brain\\n'\n",
      "                        'lukaszkaiser@google.com\\n'\n",
      "                        'Illia Polosukhin∗ ‡\\n'\n",
      "                        'illia.polosukhin@gmail.com\\n'\n",
      "                        'Abstract\\n'\n",
      "                        'The dominant sequence transduction models are based '\n",
      "                        'on complex recurrent or\\n'\n",
      "                        'convolutional neural networks that include an encoder '\n",
      "                        'and a decoder.  The best\\n'\n",
      "                        'performing models also connect the encoder and '\n",
      "                        'decoder through an attention\\n'\n",
      "                        'mechanism.  We propose a new simple network '\n",
      "                        'architecture, the Transformer,\\n'\n",
      "                        'based solely on attention mechanisms, dispensing with '\n",
      "                        'recurrence and convolutions\\n'\n",
      "                        'entirely.  Experiments on two machine translation '\n",
      "                        'tasks show these models to\\n'\n",
      "                        'be superior in quality while being more '\n",
      "                        'parallelizable and requiring significantly\\n'\n",
      "                        'less time to train.  Our model achieves 28.4 BLEU on '\n",
      "                        'the WMT 2014 English-\\n'\n",
      "                        'to-German translation task, improving over the '\n",
      "                        'existing best results, including\\n'\n",
      "                        'ensembles, by over 2 BLEU.  On the WMT 2014 '\n",
      "                        'English-to-French translation task,\\n'\n",
      "                        'our model establishes a new single-model '\n",
      "                        'state-of-the-art BLEU score of 41.8 after\\n'\n",
      "                        'training for 3.5 days on eight GPUs, a small fraction '\n",
      "                        'of the training costs of the\\n'\n",
      "                        'best models from the literature.  We show that the '\n",
      "                        'Transformer generalizes well to\\n'\n",
      "                        'other tasks by applying it successfully to English '\n",
      "                        'constituency parsing both with\\n'\n",
      "                        'large and limited training data.\\n'},\n",
      " 'metadata_separator': '\\n',\n",
      " 'metadata_seperator': '\\n',\n",
      " 'metadata_template': '{key}: {value}',\n",
      " 'mimetype': 'text/plain',\n",
      " 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9e6f4356-74e1-4d70-ba6a-38e848957fbb', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/home/bishwayansaha99/langchain/docs/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-01-03', 'last_modified_date': '2025-01-01'}, hash='efc7115257a38936524b40a73c5152171d710be198633a0988e4677c5475e258'),\n",
      "                   <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5aafd81c-7858-4eb2-8949-a2d9c09914c1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/home/bishwayansaha99/langchain/docs/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-01-03', 'last_modified_date': '2025-01-01', 'window': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\n Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder.  The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism.  We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely.  Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train.  Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU.  On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. ', 'original_text': 'We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. '}, hash='33d2f15c6319ecef243e2709bf522495b44523edcad7892d08fdc0e9c4c7a650'),\n",
      "                   <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f672b76c-9c3b-4616-8168-aec7b2bb3926', node_type=<ObjectType.TEXT: '1'>, metadata={'window': 'The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism.  We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely.  Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train.  Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU.  On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.  We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n ∗Equal contribution. ', 'original_text': 'Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. '}, hash='d91d2cd823044df01e7a1fed80dc0332417a2c4d055cc40dbef885f274c85a7a')},\n",
      " 'start_char_idx': 997,\n",
      " 'text': 'Experiments on two machine translation tasks show these models to\\n'\n",
      "         'be superior in quality while being more parallelizable and requiring '\n",
      "         'significantly\\n'\n",
      "         'less time to train. ',\n",
      " 'text_template': '{metadata_str}\\n\\n{content}'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(default_nodes[4]))\n",
    "print(\"=\" * 120)\n",
    "pprint(dict(sentence_window_nodes[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = qc.QdrantClient(\"https://35f696c3-12d7-45c8-8a79-316f7feb0e5f.us-east4-0.gcp.cloud.qdrant.io\",\n",
    "                                api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIiwiZXhwIjoxNzQ3MDY4MDIwfQ.u_uYkQP8oYn1Ck4gviEdl0RW02QKuPbgk8tN-T45PHI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"sentence-window-retriever\"\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding\n",
    "Settings.text_splitter = sentence_window_parser\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "sentence_window_index = VectorStoreIndex(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention is a function that maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum. This means that it takes into account multiple elements (keys) to generate a single representation (output), which can be thought of as focusing on specific parts of the input data that are most relevant for making predictions or generating output.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_query_engine = sentence_window_index.as_query_engine(\n",
    "    similarity_top_k = 3,\n",
    "    verbose = True,\n",
    "    node_postprocessors = [\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "sentence_query_engine.query(\"What is attention in transformer?\").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"default-retriever\"\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding\n",
    "Settings.text_splitter = default_node_parser\n",
    "sentence_window_index = VectorStoreIndex(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention is a function that maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values based on the similarity between the query and keys. This allows the model to focus on specific parts of the input data when generating output.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_query_engine = sentence_window_index.as_query_engine(\n",
    "    similarity_top_k = 3,\n",
    "    verbose = True,\n",
    "    node_postprocessors = [\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "default_query_engine.query(\"What is attention in transformer?\").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
